{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import re \n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.linear_model import Ridge\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneHotEncoder patché ? -> <class '__main__.OneHotEncoderCompat'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder as _OneHotEncoder\n",
    "\n",
    "# Wrapper de compatibilité par composition\n",
    "class OneHotEncoderCompat:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if \"sparse\" in kwargs and \"sparse_output\" not in kwargs:\n",
    "            kwargs[\"sparse_output\"] = kwargs.pop(\"sparse\")\n",
    "        self._enc = _OneHotEncoder(*args, **kwargs)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self._enc.fit(X, y)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self._enc.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self._enc.fit_transform(X, y)\n",
    "\n",
    "    def get_feature_names_out(self, *args, **kwargs):\n",
    "        return self._enc.get_feature_names_out(*args, **kwargs)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._enc, name)\n",
    "\n",
    "import neurocombat_sklearn\n",
    "neurocombat_sklearn.neurocombat_sklearn.OneHotEncoder = OneHotEncoderCompat\n",
    "from neurocombat_sklearn import CombatModel\n",
    "\n",
    "import neurocombat_sklearn.neurocombat_sklearn as ncs\n",
    "\n",
    "print(\"OneHotEncoder patché ? ->\", ncs.OneHotEncoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "labels_path = \"/neurospin/dico/rmenasria/Runs/03_main/Input/ABCD/prematurity_labels_true_classes.csv\"\n",
    "base_path  = \"/neurospin/dico/data/deep_folding/current/models/Champollion_V1_after_ablation/embeddings/ABCD_embeddings/\"\n",
    "\n",
    "\n",
    "cognition_coefs = \"/neurospin/dico/rmenasria/Runs/03_main/Output/final/prematurity/ABCD_prematurity_cog_directions.csv\"\n",
    "\n",
    "labels_df = pd.read_csv(labels_path, low_memory=False)\n",
    "labels_df['src_subject_id'] = labels_df['src_subject_id'].str.replace(\"_\",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(region):\n",
    "    \n",
    "    for file in os.listdir(base_path):\n",
    "            if file.startswith(region) and file.endswith(\".csv\"):\n",
    "                embedding_file = file\n",
    "                break\n",
    "            \n",
    "    if embedding_file is None:\n",
    "        raise FileNotFoundError(f\"No embedding file found for region: {region}\")\n",
    "\n",
    "    print(f\"Using embedding file: {embedding_file}\")\n",
    "\n",
    "    emb_path = os.path.join(base_path, embedding_file)\n",
    "    emb_df = pd.read_csv(emb_path)\n",
    "    emb_df['ID_clean'] = (\n",
    "        emb_df['ID'].astype(str)\n",
    "        .str.replace(r\"^sub-\", \"\", regex=True)\n",
    "        .str.replace(\"_\", \"\", regex=False)\n",
    "    )\n",
    "    return emb_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_sex_class_mapping(sex_series):\n",
    "\n",
    "\n",
    "    unique_sex = sorted(sex_series.unique())\n",
    "\n",
    "    #print(\"unique sites :\", unique_sex)\n",
    "\n",
    "    mapping = {1.0: 0, 2.0:1, 3.0 : 1}\n",
    "    \n",
    "    def encoder_fn(site):\n",
    "        return mapping[site]\n",
    "    \n",
    "    return mapping, encoder_fn\n",
    "\n",
    "def define_scan_age_mapping(scan_ages_series): \n",
    "\n",
    "\n",
    "    unique_scan_ages = sorted(scan_ages_series.unique())\n",
    "\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scan_ages =np.array(scan_ages_series).reshape(-1 ,1)\n",
    "    scaler.fit(scan_ages)\n",
    "    #print(\"unique_scan_ages :\",unique_scan_ages)\n",
    "\n",
    "    def encoder_fn(scan_age):\n",
    "        return scaler.transform(np.array([[scan_age]])).item()\n",
    "\n",
    "    \n",
    "    return scaler, encoder_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_300787/1431375659.py:3: DtypeWarning: Columns (307,338,352,385,397,405,436,443) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  counfounds_df = pd.read_csv(label_counfounds_path)\n",
      "/tmp/ipykernel_300787/1431375659.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  counfounds_df[\"interview_age\"].fillna(115,inplace =True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scan_age</th>\n",
       "      <th>sex</th>\n",
       "      <th>src_subject_id_clean</th>\n",
       "      <th>income_continuous</th>\n",
       "      <th>missing_income</th>\n",
       "      <th>site02</th>\n",
       "      <th>site03</th>\n",
       "      <th>site04</th>\n",
       "      <th>site05</th>\n",
       "      <th>site06</th>\n",
       "      <th>...</th>\n",
       "      <th>site13</th>\n",
       "      <th>site14</th>\n",
       "      <th>site15</th>\n",
       "      <th>site16</th>\n",
       "      <th>site17</th>\n",
       "      <th>site18</th>\n",
       "      <th>site19</th>\n",
       "      <th>site20</th>\n",
       "      <th>site21</th>\n",
       "      <th>site22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.608618</td>\n",
       "      <td>1</td>\n",
       "      <td>NDARINV003RTV85</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.939494</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARINV007W6H7B</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.934053</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARINV00BD7VDC</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.671845</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARINV00HEV6HB</td>\n",
       "      <td>8.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.201702</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARINV00J52GPG</td>\n",
       "      <td>6.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>-1.201702</td>\n",
       "      <td>1</td>\n",
       "      <td>NDARINVZZLZCKAY</td>\n",
       "      <td>9.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>-0.800228</td>\n",
       "      <td>1</td>\n",
       "      <td>NDARINVZZPKBDAC</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>0.270370</td>\n",
       "      <td>1</td>\n",
       "      <td>NDARINVZZZ2ALR6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>-1.469352</td>\n",
       "      <td>1</td>\n",
       "      <td>NDARINVZZZNB0XC</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>0.939494</td>\n",
       "      <td>1</td>\n",
       "      <td>NDARINVZZZP87KR</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9985 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      scan_age  sex src_subject_id_clean  income_continuous  missing_income  \\\n",
       "0     1.608618    1      NDARINV003RTV85                8.0           False   \n",
       "1     0.939494    0      NDARINV007W6H7B               10.0           False   \n",
       "2    -0.934053    0      NDARINV00BD7VDC               10.0           False   \n",
       "3     0.671845    0      NDARINV00HEV6HB                8.0            True   \n",
       "4    -1.201702    0      NDARINV00J52GPG                6.0           False   \n",
       "...        ...  ...                  ...                ...             ...   \n",
       "9980 -1.201702    1      NDARINVZZLZCKAY                9.0           False   \n",
       "9981 -0.800228    1      NDARINVZZPKBDAC               10.0           False   \n",
       "9982  0.270370    1      NDARINVZZZ2ALR6               10.0           False   \n",
       "9983 -1.469352    1      NDARINVZZZNB0XC                3.0           False   \n",
       "9984  0.939494    1      NDARINVZZZP87KR                7.0           False   \n",
       "\n",
       "      site02  site03  site04  site05  site06  ...  site13  site14  site15  \\\n",
       "0      False   False   False   False    True  ...   False   False   False   \n",
       "1      False   False   False   False   False  ...   False   False   False   \n",
       "2      False   False   False   False   False  ...   False   False   False   \n",
       "3      False   False   False   False   False  ...   False   False   False   \n",
       "4      False   False   False   False   False  ...   False   False   False   \n",
       "...      ...     ...     ...     ...     ...  ...     ...     ...     ...   \n",
       "9980   False   False   False   False    True  ...   False   False   False   \n",
       "9981   False   False   False   False   False  ...   False   False   False   \n",
       "9982   False   False   False   False   False  ...   False   False   False   \n",
       "9983   False    True   False   False   False  ...   False   False   False   \n",
       "9984   False   False   False   False   False  ...   False   False   False   \n",
       "\n",
       "      site16  site17  site18  site19  site20  site21  site22  \n",
       "0      False   False   False   False   False   False   False  \n",
       "1      False   False   False   False   False   False    True  \n",
       "2      False   False   False   False   False   False   False  \n",
       "3      False   False   False   False   False   False   False  \n",
       "4      False    True   False   False   False   False   False  \n",
       "...      ...     ...     ...     ...     ...     ...     ...  \n",
       "9980   False   False   False   False   False   False   False  \n",
       "9981   False   False   False   False   False   False   False  \n",
       "9982   False   False   False   False   False   False   False  \n",
       "9983   False   False   False   False   False   False   False  \n",
       "9984   False   False   False    True   False   False   False  \n",
       "\n",
       "[9985 rows x 26 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_confound_df(label_counfounds_path = \"/neurospin/dico/rmenasria/Runs/03_main/Input/ABCD/all_labels_clean_abcd_new_classes.csv\",income_df_path = \"/neurospin/dico/rmenasria/Runs/03_main/Input/ABCD/income.csv\"):\n",
    "    \n",
    "    counfounds_df = pd.read_csv(label_counfounds_path)\n",
    "    counfounds_df = pd.get_dummies(counfounds_df,columns = ['site_id_l'],prefix='',prefix_sep='',drop_first=True)\n",
    "    income_df = pd.read_csv(income_df_path)\n",
    "    income_df['src_subject_id_clean'] = income_df['src_subject_id'].str.replace(\"_\",\"\")\n",
    "    #keep only necessary columns\n",
    "    income_df = income_df[['src_subject_id_clean','income_continuous','missing_income']]\n",
    "\n",
    "\n",
    "    mapping, encode_site = define_sex_class_mapping(counfounds_df['demo_sex_v2'])\n",
    "    counfounds_df['sex'] = counfounds_df['demo_sex_v2'].apply(encode_site)\n",
    "    counfounds_df.drop(columns=['demo_sex_v2'],inplace=True)\n",
    "\n",
    "\n",
    "    counfounds_df[\"interview_age\"].fillna(115,inplace =True)\n",
    "\n",
    "    scaler, encode_scan_age = define_scan_age_mapping(counfounds_df[\"interview_age\"])\n",
    "\n",
    "    counfounds_df['scan_age']= counfounds_df['interview_age'].apply(encode_scan_age)\n",
    "\n",
    "    counfounds_df = counfounds_df.merge(income_df,left_on='src_subject_id_clean',how='left',right_on='src_subject_id_clean')\n",
    "\n",
    "\n",
    "    # Filter to keep only necessary columns\n",
    "    columns_to_keep = ['scan_age','sex','src_subject_id_clean','income_continuous','missing_income'] + [col for col in counfounds_df.columns if col.startswith('site')]\n",
    "    counfounds_df = counfounds_df[columns_to_keep]\n",
    "\n",
    "    #print(\"counfounds_df columns :\",counfounds_df.columns) \n",
    "\n",
    "    return counfounds_df\n",
    "\n",
    "\n",
    "set_confound_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _derive_site_from_onehot(df, pattern=r'^site\\d{2}$', missing_label='site_missing'):\n",
    "    site_cols = [c for c in df.columns if re.match(pattern, c)]\n",
    "    if not site_cols:\n",
    "        return None, []\n",
    "    arg = df[site_cols].astype(float).values\n",
    "    ix = np.argmax(arg, axis=1)\n",
    "    site_series = pd.Series(\n",
    "        [site_cols[i] if (arg[row, i] == 1.0) else missing_label\n",
    "         for row, i in enumerate(ix)],\n",
    "        index=df.index, name='site_cat'\n",
    "    )\n",
    "    mask_all_zero = (arg.sum(axis=1) == 0)\n",
    "    site_series.loc[mask_all_zero] = missing_label\n",
    "    return site_series, site_cols\n",
    "\n",
    "def _collapse_rare_sites(site_array, min_size):\n",
    "    counts = Counter(site_array)\n",
    "    return np.array([s if counts[s] >= min_size else \"site_other\" for s in site_array])\n",
    "\n",
    "def prepare_cv_data(\n",
    "    embeddings_df,\n",
    "    labels_df,\n",
    "    confounds_df,\n",
    "    id_emb_col='ID_clean',\n",
    "    id_label_col='src_subject_id',\n",
    "    id_conf_col='src_subject_id_clean',\n",
    "    confounds_for_strat=[],\n",
    "    confounds_for_resid=None,\n",
    "    threshold=\"28-32\",\n",
    "    dim_regex=r'^dim',\n",
    "    n_splits=5,\n",
    "    site_onehot_regex=r'^site\\d{2}$',\n",
    "    min_site_size=50  \n",
    "):\n",
    "    emb = embeddings_df.copy()\n",
    "    lab = labels_df.copy()\n",
    "    cf  = confounds_df.copy()\n",
    "\n",
    "    emb[id_emb_col]   = emb[id_emb_col].astype(str)\n",
    "    lab[id_label_col] = lab[id_label_col].astype(str)\n",
    "    cf[id_conf_col]   = cf[id_conf_col].astype(str)\n",
    "\n",
    "    merged = emb.merge(lab, left_on=id_emb_col, right_on=id_label_col, how='inner')\n",
    "    df = merged.merge(cf, left_on=id_emb_col, right_on=id_conf_col, how='inner', suffixes=('','_cf'))\n",
    "\n",
    "    # cible binaire\n",
    "    df = df[df['prem_class'].isin([\"<28\",\"28-32\",\"32-37\"])].copy()\n",
    "    df['y'] = (df['prem_class'].isin([\"<28\",\"28-32\",\"32-37\"])).astype(int)\n",
    "\n",
    "    # reconstruire l’étiquette de site à partir du one-hot\n",
    "    site_cat, site_cols = _derive_site_from_onehot(df, pattern=site_onehot_regex)\n",
    "    if site_cat is None:\n",
    "        raise ValueError(f\"Aucune colonne de site détectée avec le regex {site_onehot_regex}.\")\n",
    "    df['site_cat'] = site_cat.astype(str)\n",
    "\n",
    "    # collapse des sites trop petits\n",
    "    if min_site_size is None:\n",
    "        min_site_size = n_splits  # par défaut : au moins n_splits sujets\n",
    "    df['site_cat'] = _collapse_rare_sites(df['site_cat'].values, min_site_size)\n",
    "\n",
    "    # X / y / ids\n",
    "    X_all_df = df.filter(regex=dim_regex)\n",
    "    X_all = X_all_df.to_numpy(dtype=float)\n",
    "    y_all = df['y'].to_numpy(dtype=int)\n",
    "    ids_all = df[id_label_col].astype(str).to_numpy()\n",
    "\n",
    "    # confounds pour résidualisation\n",
    "    if confounds_for_resid is None:\n",
    "        confounds_for_resid = confounds_df.columns.tolist()\n",
    "        if id_conf_col in confounds_for_resid:\n",
    "            confounds_for_resid.remove(id_conf_col)\n",
    "    confounds_for_resid = [c for c in confounds_for_resid if c not in site_cols and c != 'site_cat']\n",
    "    confounds_resid_df = df[confounds_for_resid].copy()\n",
    "\n",
    "    # stratification : site uniquement (y peut être ignoré si tu n’as que des prémas)\n",
    "    confounds_for_strat = list(confounds_for_strat) if confounds_for_strat is not None else []\n",
    "    if 'site_cat' not in confounds_for_strat:\n",
    "        confounds_for_strat.append('site_cat')\n",
    "\n",
    "    stratify_labels = df['site_cat'].astype(str).to_numpy()\n",
    "\n",
    "    # count occurrences\n",
    "    counts = Counter(stratify_labels)\n",
    "    print(\"Occurrences par strata pour stratification :\", counts)\n",
    "\n",
    "    return {\n",
    "        'df_complete': df,\n",
    "        'X_all_df': X_all_df,\n",
    "        'X_all': X_all,\n",
    "        'y_all': y_all,\n",
    "        'ids_all': ids_all,\n",
    "        'stratify_labels': stratify_labels,\n",
    "        'confounds_resid_df': confounds_resid_df,\n",
    "        'site_array': df['site_cat'].astype(str).to_numpy(),\n",
    "        'site_onehot_cols': site_cols,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residualize_in_folds_from_prep_combat_final(\n",
    "    prep,\n",
    "    confounds_list=None,\n",
    "    id_label_col='src_subject_id',\n",
    "    id_emb_col='ID_clean',\n",
    "    dim_regex=r'^dim',\n",
    "    n_splits=5,\n",
    "    random_state=42,\n",
    "    use_statsmodels=True,\n",
    "    use_combat=True,\n",
    "):\n",
    "    merged_df = prep['df_complete'].copy()\n",
    "    X_all = prep['X_all']; y_all = prep['y_all']; ids_all = prep['ids_all']\n",
    "    stratify_labels = prep['stratify_labels']\n",
    "    confounds_df = prep['confounds_resid_df']\n",
    "\n",
    "    # confounds list (déjà propres depuis prepare_cv_data) \n",
    "    if confounds_list is None:\n",
    "        confounds_list = list(confounds_df.columns)  # aligné aux sujets filtrés\n",
    "        # retire les id-like si présents\n",
    "        confounds_list = [c for c in confounds_list if not re.search(r'(src|id)', c, flags=re.I)]\n",
    "\n",
    "    # colonnes site à exclure de l'OLS (toutes les one-hot + site_cat)\n",
    "    site_onehot_cols = [c for c in merged_df.columns if re.match(r'^site\\d{2}$', c)]\n",
    "    site_like_cols = set(site_onehot_cols + (['site_cat'] if 'site_cat' in merged_df.columns else []))\n",
    "    confounds_list = [c for c in confounds_list if c not in site_like_cols]\n",
    "\n",
    "    # split confounds en continu / discret (si tu en as besoin pour ComBat)\n",
    "    preferred_cont = ['scan_age', 'income_continuous']\n",
    "    preferred_disc = ['missing_income', 'sex']\n",
    "    confounds_continuous = [c for c in preferred_cont if c in merged_df.columns and c in confounds_list]\n",
    "    confounds_discrete  = [c for c in preferred_disc if c in merged_df.columns and c in confounds_list]\n",
    "    remaining = [c for c in confounds_list if c not in (confounds_continuous + confounds_discrete)]\n",
    "    confounds_continuous += remaining  # par défaut en continu\n",
    "\n",
    "    # site/batch pour ComBat\n",
    "    if use_combat:\n",
    "        if 'site_array' in prep:\n",
    "            site_series = pd.Series(prep['site_array'], index=merged_df.index).astype(str)\n",
    "        else:\n",
    "            # fallback: reconstruire depuis le one-hot si jamais\n",
    "            if not site_onehot_cols:\n",
    "                raise RuntimeError(\"Aucune info de site trouvée (ni site_array, ni siteXX).\")\n",
    "            ix = np.argmax(merged_df[site_onehot_cols].astype(float).values, axis=1)\n",
    "            site_series = pd.Series([site_onehot_cols[i] for i in ix], index=merged_df.index, dtype=str)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        batch_all = le.fit_transform(site_series.values)  # ex: 'site02' -> 0..K-1\n",
    "    else:\n",
    "        batch_all = None\n",
    "\n",
    "    # dims\n",
    "    dim_cols = merged_df.filter(regex=dim_regex).columns.tolist()\n",
    "\n",
    "    # K-fold (strat sur y||site_cat||confounds binned) \n",
    "    outer = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    folds_residualized = []\n",
    "    id_to_y = dict(zip(ids_all.astype(str), y_all.astype(int)))\n",
    "\n",
    "    for fold_i, (train_idx, test_idx) in enumerate(outer.split(X_all, stratify_labels)):\n",
    "        train_ids = ids_all[train_idx].astype(str); test_ids = ids_all[test_idx].astype(str)\n",
    "\n",
    "        merged_df[id_emb_col] = merged_df[id_emb_col].astype(str)\n",
    "        train_mask = merged_df[id_emb_col].isin(train_ids)\n",
    "        test_mask  = merged_df[id_emb_col].isin(test_ids)\n",
    "\n",
    "        X_train_emb_df = merged_df.loc[train_mask, dim_cols].astype(float)\n",
    "        X_test_emb_df  = merged_df.loc[test_mask,  dim_cols].astype(float)\n",
    "\n",
    "        # ComBat (fit sur train, transform sur test) \n",
    "        if use_combat:\n",
    "            # covariables continues (remplissage par stats train)\n",
    "            if len(confounds_continuous) > 0:\n",
    "                cont_train_df = merged_df.loc[train_mask, confounds_continuous].astype(float).copy()\n",
    "                cont_test_df  = merged_df.loc[test_mask,  confounds_continuous].astype(float).copy()\n",
    "                cont_train_df = cont_train_df.fillna(cont_train_df.median())\n",
    "                cont_test_df  = cont_test_df.fillna(cont_train_df.median())\n",
    "                continuous_train_arr = cont_train_df.values\n",
    "                continuous_test_arr  = cont_test_df.values\n",
    "            else:\n",
    "                continuous_train_arr = None; continuous_test_arr = None\n",
    "\n",
    "            # covariables discrètes\n",
    "            if len(confounds_discrete) > 0:\n",
    "                disc_train_df = merged_df.loc[train_mask, confounds_discrete].copy()\n",
    "                disc_test_df  = merged_df.loc[test_mask,  confounds_discrete].copy()\n",
    "                for c in disc_train_df.columns:\n",
    "                    mv = disc_train_df[c].mode().iloc[0]\n",
    "                    disc_train_df[c] = disc_train_df[c].fillna(mv)\n",
    "                    disc_test_df[c]  = disc_test_df[c].fillna(mv)\n",
    "            else:\n",
    "                disc_train_df = pd.DataFrame(index=X_train_emb_df.index)\n",
    "                disc_test_df  = pd.DataFrame(index=X_test_emb_df.index)\n",
    "\n",
    "            # préserver y (en discrète) pour éviter de l’écraser\n",
    "            # y_train_list = [id_to_y[i] for i in merged_df.loc[train_mask, id_emb_col]]\n",
    "            # y_test_list  = [id_to_y[i] for i in merged_df.loc[test_mask,  id_emb_col]]\n",
    "            # disc_train_df = pd.concat([\n",
    "            #     disc_train_df.reset_index(drop=True),\n",
    "            #     pd.Series(y_train_list, name='__y_preserve')\n",
    "            # ], axis=1).set_index(X_train_emb_df.index)\n",
    "            # disc_test_df = pd.concat([\n",
    "            #     disc_test_df.reset_index(drop=True),\n",
    "            #     pd.Series(y_test_list, name='__y_preserve')\n",
    "            # ], axis=1).set_index(X_test_emb_df.index)\n",
    "\n",
    "            discrete_train_arr = disc_train_df.values if disc_train_df.shape[1] > 0 else None\n",
    "            discrete_test_arr  = disc_test_df.values  if disc_test_df.shape[1] > 0 else None\n",
    "\n",
    "            batch_train = batch_all[train_mask.values].reshape(-1, 1)\n",
    "            batch_test  = batch_all[test_mask.values].reshape(-1, 1)\n",
    "\n",
    "            # sécurité : tout site présent en test doit exister en train\n",
    "            miss = np.setdiff1d(np.unique(batch_test), np.unique(batch_train))\n",
    "            if miss.size:\n",
    "                raise RuntimeError(f\"Fold {fold_i}: site(s) uniquement en test → {miss}. \"\n",
    "                                   f\"Assure-toi que 'site_cat' est inclus dans stratify_labels.\")\n",
    "\n",
    "            combat = CombatModel()\n",
    "            X_train_h = combat.fit_transform(X_train_emb_df.values, batch_train, discrete_train_arr, continuous_train_arr)\n",
    "            X_test_h  = combat.transform(X_test_emb_df.values,  batch_test,  discrete_test_arr,  continuous_test_arr)\n",
    "\n",
    "            X_train_h_df = pd.DataFrame(X_train_h, index=X_train_emb_df.index, columns=dim_cols)\n",
    "            X_test_h_df  = pd.DataFrame(X_test_h,  index=X_test_emb_df.index,  columns=dim_cols)\n",
    "        else:\n",
    "            X_train_h_df = X_train_emb_df.copy()\n",
    "            X_test_h_df  = X_test_emb_df.copy()\n",
    "\n",
    "        # OLS residualization (sur embeddings harmonisés) \n",
    "        conf_train_df = merged_df.loc[train_mask, confounds_list].copy() if len(confounds_list)>0 else pd.DataFrame(index=X_train_h_df.index)\n",
    "        conf_test_df  = merged_df.loc[test_mask,  confounds_list].copy() if len(confounds_list)>0 else pd.DataFrame(index=X_test_h_df.index)\n",
    "\n",
    "        if conf_train_df.shape[1] > 0:\n",
    "            for c in conf_train_df.columns:\n",
    "                if pd.api.types.is_numeric_dtype(conf_train_df[c]):\n",
    "                    med = conf_train_df[c].median()\n",
    "                    conf_train_df[c] = conf_train_df[c].fillna(med)\n",
    "                    conf_test_df[c]  = conf_test_df[c].fillna(med)\n",
    "                else:\n",
    "                    mv = conf_train_df[c].mode().iloc[0]\n",
    "                    conf_train_df[c] = conf_train_df[c].fillna(mv)\n",
    "                    conf_test_df[c]  = conf_test_df[c].fillna(mv)\n",
    "        else:\n",
    "            conf_train_df = pd.DataFrame(index=X_train_h_df.index)\n",
    "            conf_test_df  = pd.DataFrame(index=X_test_h_df.index)\n",
    "\n",
    "        if use_statsmodels:\n",
    "            import statsmodels.api as sm\n",
    "            X_conf_train = sm.add_constant(conf_train_df.astype(float), has_constant='add')\n",
    "            X_conf_test  = sm.add_constant(conf_test_df.astype(float),  has_constant='add')\n",
    "            resid_train_df = pd.DataFrame(index=X_train_h_df.index, columns=dim_cols, dtype=float)\n",
    "            resid_test_df  = pd.DataFrame(index=X_test_h_df.index,  columns=dim_cols, dtype=float)\n",
    "            for col in dim_cols:\n",
    "                y_tr_col = X_train_h_df[col].astype(float)\n",
    "                if X_conf_train.shape[1] == 0:\n",
    "                    resid_train_df[col] = y_tr_col.values\n",
    "                    resid_test_df[col]  = X_test_h_df[col].astype(float).values\n",
    "                else:\n",
    "                    ols = sm.OLS(y_tr_col, X_conf_train).fit()\n",
    "                    resid_train_df[col] = y_tr_col - ols.predict(X_conf_train)\n",
    "                    resid_test_df[col]  = X_test_h_df[col].astype(float) - ols.predict(X_conf_test)\n",
    "        else:\n",
    "            if conf_train_df.shape[1] == 0:\n",
    "                resid_train_df = X_train_h_df.copy()\n",
    "                resid_test_df  = X_test_h_df.copy()\n",
    "            else:\n",
    "                lr = LinearRegression()\n",
    "                lr.fit(conf_train_df.values, X_train_h_df.values)\n",
    "                resid_train_df = pd.DataFrame(X_train_h_df.values - lr.predict(conf_train_df.values),\n",
    "                                              index=X_train_h_df.index, columns=dim_cols)\n",
    "                resid_test_df  = pd.DataFrame(X_test_h_df.values  - lr.predict(conf_test_df.values),\n",
    "                                              index=X_test_h_df.index,  columns=dim_cols)\n",
    "\n",
    "        # y aligné\n",
    "        train_ids_present = merged_df.loc[train_mask, id_emb_col].astype(str).tolist()\n",
    "        test_ids_present  = merged_df.loc[test_mask,  id_emb_col].astype(str).tolist()\n",
    "        y_train = np.array([id_to_y[i] for i in train_ids_present], dtype=int)\n",
    "        y_test  = np.array([id_to_y[i] for i in test_ids_present],  dtype=int)\n",
    "\n",
    "        folds_residualized.append({\n",
    "            'X_train_resid': resid_train_df.values,\n",
    "            'X_test_resid' : resid_test_df.values,\n",
    "            'y_train': y_train, 'y_test': y_test,\n",
    "            'train_ids': train_ids_present, 'test_ids': test_ids_present\n",
    "        })\n",
    "\n",
    "    # Full-data (pour refit/visu)\n",
    "    merged_df_h = merged_df.copy()\n",
    "    if use_combat:\n",
    "        # mêmes covariables que plus haut (fill par stats globales)\n",
    "        if len(confounds_continuous) > 0:\n",
    "            full_cont_df = merged_df[confounds_continuous].astype(float).copy()\n",
    "            full_cont_df = full_cont_df.fillna(full_cont_df.median())\n",
    "            continuous_all_arr = full_cont_df.values\n",
    "        else:\n",
    "            continuous_all_arr = None\n",
    "\n",
    "        if len(confounds_discrete) > 0:\n",
    "            full_disc_df = merged_df[confounds_discrete].copy()\n",
    "            for c in full_disc_df.columns:\n",
    "                mv = full_disc_df[c].mode().iloc[0]\n",
    "                full_disc_df[c] = full_disc_df[c].fillna(mv)\n",
    "        else:\n",
    "            full_disc_df = pd.DataFrame(index=merged_df.index)\n",
    "\n",
    "        # full_disc_df = pd.concat([full_disc_df.reset_index(drop=True),\n",
    "        #                           pd.Series(y_all.astype(int), name='__y_preserve')], axis=1).set_index(merged_df.index)\n",
    "        discrete_all_arr = full_disc_df.values if full_disc_df.shape[1] > 0 else None\n",
    "\n",
    "        batches_full = batch_all.reshape(-1, 1)\n",
    "        combat_full = CombatModel()\n",
    "        X_all_h = combat_full.fit_transform(merged_df[dim_cols].values, batches_full, discrete_all_arr, continuous_all_arr)\n",
    "        merged_df_h[dim_cols] = X_all_h\n",
    "\n",
    "    # OLS full\n",
    "    confounds_for_ols_full = list(confounds_list)  # déjà sans sites\n",
    "    if len(confounds_for_ols_full) == 0:\n",
    "        merged_df_resid = merged_df_h.copy()\n",
    "    else:\n",
    "        full_conf_df = merged_df.loc[:, confounds_for_ols_full].copy()\n",
    "        for c in full_conf_df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(full_conf_df[c]):\n",
    "                full_conf_df[c] = full_conf_df[c].fillna(full_conf_df[c].median())\n",
    "            else:\n",
    "                full_conf_df[c] = full_conf_df[c].fillna(full_conf_df[c].mode().iloc[0])\n",
    "        lr_full = LinearRegression().fit(full_conf_df.values, merged_df_h[dim_cols].values)\n",
    "        X_all_resid = merged_df_h[dim_cols].values - lr_full.predict(full_conf_df.values)\n",
    "        merged_df_resid = merged_df_h.copy(); merged_df_resid[dim_cols] = X_all_resid\n",
    "\n",
    "    folds_residualized.append({\n",
    "        'X_all_resid': merged_df_resid.filter(regex=dim_regex).values,\n",
    "        'y_all': y_all, 'ids_all': ids_all, 'df_resid': merged_df_resid\n",
    "    })\n",
    "    return folds_residualized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_on_direction(X, coef, intercept):\n",
    "    \"\"\"\n",
    "    Projects the data X onto the hyperplane defined by coef and intercept.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The input data to be projected.\n",
    "    coef : array-like, shape (n_features,)\n",
    "        The coefficients defining the hyperplane.\n",
    "    intercept : float\n",
    "        The intercept defining the hyperplane.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    projections : array, shape (n_samples,)\n",
    "        The projections of each sample onto the hyperplane.\n",
    "    \"\"\"\n",
    "    # Calculate the norm of the coefficient vector\n",
    "    norm_coef = np.linalg.norm(coef)\n",
    "\n",
    "    # Project each sample onto the hyperplane\n",
    "    projections = (np.dot(X, coef) + intercept) / norm_coef\n",
    "\n",
    "    return projections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_residualized_embeddings(region, output_path, labels_df, alpha=10.0):\n",
    "    embeddings_df = load_embeddings(region)\n",
    "    confounds_df = set_confound_df()\n",
    "    preps = prepare_cv_data(embeddings_df, labels_df, confounds_df)\n",
    "    folds_resids = residualize_in_folds_from_prep_combat_final(preps)\n",
    "\n",
    "    selected_item = folds_resids[-1]\n",
    "    X_all_resid = selected_item['X_all_resid']\n",
    "    ids_all = selected_item['ids_all']\n",
    "    y_all = selected_item['y_all']\n",
    "\n",
    "    X_all_resid_df = pd.DataFrame(\n",
    "        X_all_resid,\n",
    "        index=ids_all,\n",
    "        columns=preps['X_all_df'].columns\n",
    "    )\n",
    "\n",
    "    # save to csv\n",
    "    X_all_resid_df.reset_index(inplace=True)\n",
    "    X_all_resid_df.rename(columns={'index': 'ID_clean'}, inplace=True)\n",
    "    \n",
    "    out_csv = output_path + f\"_{region}_resid.csv\"\n",
    "    X_all_resid_df.to_csv(out_csv, index=False)\n",
    "\n",
    "    print(f\"Saved residualized embeddings for region {region} to {out_csv}\")\n",
    "\n",
    "    # --- fit final ridge model on residualized embeddings ---\n",
    "    final_pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Ridge(alpha=alpha))\n",
    "    ])\n",
    "    final_pipe.fit(X_all_resid, y_all)\n",
    "\n",
    "    reg, sc = final_pipe.named_steps['regressor'], final_pipe.named_steps['scaler']\n",
    "    w_x = reg.coef_.ravel() / sc.scale_\n",
    "    b_x = float(reg.intercept_) - np.dot(w_x, sc.mean_)\n",
    "\n",
    "    return {\n",
    "        \"X_all_resid\": X_all_resid,\n",
    "        \"y_all\": y_all,\n",
    "        \"ids_all\": ids_all,\n",
    "        \"out_csv\": out_csv,\n",
    "        \"w_x\": w_x,\n",
    "        \"b_x\": b_x\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding file: ScCal-SLi_left_name07-41-43--180_embeddings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_300787/1431375659.py:3: DtypeWarning: Columns (307,338,352,385,397,405,436,443) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  counfounds_df = pd.read_csv(label_counfounds_path)\n",
      "/tmp/ipykernel_300787/1431375659.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  counfounds_df[\"interview_age\"].fillna(115,inplace =True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences par strata pour stratification : Counter({'site_other': 388, 'site20': 197, 'site14': 192, 'site02': 182, 'site19': 165, 'site03': 61, 'site10': 52, 'site16': 51, 'site13': 51})\n",
      "Saved residualized embeddings for region ScCal-SLi_left to /neurospin/dico/rmenasria/Runs/03_main/Output/embeddings_residualized/cognition_ScCal-SLi_left_resid.csv\n"
     ]
    }
   ],
   "source": [
    "region = \"ScCal-SLi_left\"\n",
    "output_path = \"/neurospin/dico/rmenasria/Runs/03_main/Output/embeddings_residualized/cognition\"\n",
    "save_residualized_embeddings(region, output_path, labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef shape: (32,)\n",
      "intercept: 105.60995862277544\n",
      "Saved sorted directions for region STi-SOTlat_right, score nihtbx_picvocab_agecorrected to /neurospin/dico/rmenasria/Runs/03_main/Output/final_direction/prema_cognition/STi-SOTlat_right_nihtbx_picvocab_agecorrected_direction.csv\n"
     ]
    }
   ],
   "source": [
    "def parse_numpy_like_string(s):\n",
    "    # enlève les crochets\n",
    "    s = s.strip().lstrip(\"[\").rstrip(\"]\")\n",
    "    # découpe sur les espaces\n",
    "    nums = re.split(r\"\\s+\", s.strip())\n",
    "    # convertit en float\n",
    "    return np.array([float(x) for x in nums if x != \"\"])\n",
    "\n",
    "df_dir_28_32 = pd.read_csv(cognition_coefs)\n",
    "\n",
    "def save_directions(region, score, output_path):\n",
    "    global df_dir_28_32\n",
    "\n",
    "    resid_path = f\"/neurospin/dico/rmenasria/Runs/03_main/Output/embeddings_residualized/cognition/prema_cognition_{region}_resid.csv\"\n",
    "    resid_df = pd.read_csv(resid_path)\n",
    "\n",
    "    # sélection correcte : filtrer à la fois sur région ET score\n",
    "    dir_row = df_dir_28_32[(df_dir_28_32['region'] == region) & (df_dir_28_32['score'] == score)]\n",
    "    if dir_row.empty:\n",
    "        raise ValueError(f\"No direction found for region: {region}, score: {score}\")\n",
    "\n",
    "    coef_str = dir_row['final_coef_original'].values[0]\n",
    "    intercept_val = dir_row['final_intercept_original'].values[0]\n",
    "\n",
    "    # parse coefficients\n",
    "    coef = parse_numpy_like_string(coef_str)\n",
    "\n",
    "    # intercept peut être string avec [] ou déjà un float\n",
    "    if isinstance(intercept_val, str):\n",
    "        intercept = parse_numpy_like_string(intercept_val).item()\n",
    "    else:\n",
    "        intercept = float(intercept_val)\n",
    "\n",
    "    print(\"coef shape:\", coef.shape)\n",
    "    print(\"intercept:\", intercept)\n",
    "\n",
    "    # compute projections\n",
    "    projections = project_on_direction(\n",
    "        resid_df.filter(regex=r'^dim').values, coef, intercept\n",
    "    )\n",
    "\n",
    "    # build DataFrame sorted by projection (descending = most preterm first)\n",
    "    directions_df = pd.DataFrame({\n",
    "        'ID_clean': resid_df['ID_clean'],\n",
    "        'projection': projections\n",
    "    }).sort_values(by='projection', ascending=False)\n",
    "\n",
    "    output_file = os.path.join(output_path, f\"{region}_{score}_direction.csv\")\n",
    "    directions_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Saved sorted directions for region {region}, score {score} to {output_file}\")\n",
    "\n",
    "\n",
    "region = \"STi-SOTlat_right\"\n",
    "output_path = \"/neurospin/dico/rmenasria/Runs/03_main/Output/final_direction/prema_cognition\"\n",
    "save_directions(region,\"nihtbx_picvocab_agecorrected\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding file: STi-SOTlat_right_name06-17-38--74_embeddings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_300787/1431375659.py:3: DtypeWarning: Columns (307,338,352,385,397,405,436,443) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  counfounds_df = pd.read_csv(label_counfounds_path)\n",
      "/tmp/ipykernel_300787/1431375659.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  counfounds_df[\"interview_age\"].fillna(115,inplace =True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences par strata pour stratification : Counter({'site_other': 388, 'site20': 197, 'site14': 192, 'site02': 182, 'site19': 165, 'site03': 61, 'site10': 52, 'site16': 51, 'site13': 51})\n",
      "Saved residualized embeddings for region STi-SOTlat_right to /neurospin/dico/rmenasria/Runs/03_main/Output/final_direction/prema_cognition_STi-SOTlat_right_resid.csv\n",
      "Sanity check:\n",
      "  Pipeline preds  -> min: 1.0 max: 1.0 mean: 1.0\n",
      "  Direct preds    -> min: 1.0 max: 1.0 mean: 1.0\n",
      "  Diff (MSE): 0.0\n",
      "✅ OK : les coef/intercept corrigés sont cohérents avec le pipeline.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "\n",
    "def check_equivalence(X_all_resid, y_all, best_alpha, coef, intercept):\n",
    "    \"\"\"\n",
    "    Vérifie que les coefficients/intercept corrigés reproduisent bien\n",
    "    les prédictions du pipeline [StandardScaler + Ridge].\n",
    "    \"\"\"\n",
    "\n",
    "    # Refit pipeline standard\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('regressor', Ridge(alpha=best_alpha))])\n",
    "    pipe.fit(X_all_resid, y_all)\n",
    "\n",
    "    # Prédictions pipeline (avec scaler)\n",
    "    y_pred_pipe = pipe.predict(X_all_resid)\n",
    "\n",
    "    # Prédictions via coef/intercept \"corrigés\"\n",
    "    y_pred_direct = X_all_resid @ coef + intercept\n",
    "\n",
    "    # Comparaison\n",
    "    print(\"Sanity check:\")\n",
    "    print(\"  Pipeline preds  -> min:\", y_pred_pipe.min(), \"max:\", y_pred_pipe.max(), \"mean:\", y_pred_pipe.mean())\n",
    "    print(\"  Direct preds    -> min:\", y_pred_direct.min(), \"max:\", y_pred_direct.max(), \"mean:\", y_pred_direct.mean())\n",
    "    print(\"  Diff (MSE):\", np.mean((y_pred_pipe - y_pred_direct) ** 2))\n",
    "\n",
    "    # Vérif stricte : doivent être quasi-identiques\n",
    "    if np.allclose(y_pred_pipe, y_pred_direct, rtol=1e-6, atol=1e-6):\n",
    "        print(\"OK : les coef/intercept corrigés sont cohérents avec le pipeline.\")\n",
    "    else:\n",
    "        print(\"Attention : il y a une différence, vérifier la sauvegarde des coef/intercept.\")\n",
    "\n",
    "\n",
    "\n",
    "res = save_residualized_embeddings(region, output_path, labels_df)\n",
    "check_equivalence(res[\"X_all_resid\"], res[\"y_all\"], 0.1, res[\"w_x\"], res[\"b_x\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
